a) Alguns Comandos (atalhos)
	ctrl + shift + t: nova janela terminal
	ctrl + shift + e: nova janela terminal à esquerda
	ctrl + shift + o: nova janela terminal embaixo
	ctrl+ pagup/pgdn: trocar de abas no terminal

b) Running Kafka docker local
Dowload docker-kafka:
	https://hub.docker.com/r/spotify/kafka/
Dowload Kafka:
	http://mirror.nbtelecom.com.br/apache/kafka/1.1.1/kafka-1.1.1-src.tgz


	1 - docker run -p 2181:2181 -p 9092:9092 --env ADVERTISED_HOST='localhost' --env ADVERTISED_PORT=9092 spotify/kafka

	2 - Iniciando Producer:
		~/git/Kafka/kafka_2.11-1.1.0/bin$ ./kafka-console-producer.sh --broker-list localhost:9092 --topic test

	3 - Iniciando Consumer:
		~/git/Kafka/kafka_2.11-1.1.0/bin$ ./kafka-console-consumer.sh --zookeeper localhost:2181 --topic test

		OBS: Ctrl+C  para sair

c) Conecção Arduino-Python/Kafka
	
	1 - Instalar e importar as bibliotecas necessárias
	pip install kafka-python
	from kafka import KafkaProducer

	2 - Instalar IDE Arduino e code
		Para run o code arduino (após entrar no diretório do file) sudo python voltage_collector.py 
		Lembrar de verificar a porta de conexão com o arduino

	3 - No code do python
		definir producer: producer = KafkaProducer(bootstrap_servers='localhost:9092')
		enviar msg: producer.send('test', str(value))

d) Conexão Kafka-Spark, através do spark streaming

	- Preparar ambiente de dev: install IntelliJ IDEA (sbt e scala podem ser instalados direto no intellij)
		Dependências (em build.sbt):
			name := "HelloScala"
			version := "0.1"
			scalaVersion := "2.11.12"
			libraryDependencies ++= Seq("org.apache.spark" % "spark-core_2.11" % "2.2.0",
			  "org.scala-lang" % "scala-library" % "2.11.12",
			  "org.apache.spark" % "spark-streaming_2.11" % "2.2.0",
			  "org.apache.spark" % "spark-streaming-kafka-0-10_2.11" % "2.2.0",
			  "org.elasticsearch" % "elasticsearch-spark-20_2.11" % "6.2.4",
			  "org.json4s" % "json4s-native_2.11" % "3.2.9",
			  "org.json4s" % "json4s-jackson_2.11" %  "3.2.9"
)
	- Spark Streaming: code do processing
			import org.apache.spark.{SparkConf}
			import org.apache.kafka.common.serialization.StringDeserializer
			import org.apache.spark.streaming.kafka010._
			import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
			import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
			import org.apache.spark.streaming._
			import org.json4s._
			import org.json4s.native.JsonMethods._
			import org.elasticsearch.spark.rdd.EsSpark

			object HelloScala {

			  def main(args: Array[String]): Unit = {
			    println("HelloWorld!")

			    //Create a SparkConf to initialize Spark
			    val conf = new SparkConf()
			    conf.setMaster("local")
			    conf.setAppName("dracarys")
			    conf.set("es.index.auto.create", "true")
			    conf.set("es.batch.size.entries", "1000")

			    val kafkaParams = Map[String, Object](
			      "bootstrap.servers" -> "localhost:9092",
			      "key.deserializer" -> classOf[StringDeserializer],
			      "value.deserializer" -> classOf[StringDeserializer],
			      "group.id" -> "sparkStream",
			      "auto.offset.reset" -> "earliest",
			      "enable.auto.commit" -> (false: java.lang.Boolean)
			    )
			    val ssc = new StreamingContext(conf, Seconds(10))
			    val topics = Array("test")
			    val stream = KafkaUtils.createDirectStream[String, String](
			      ssc,
			      PreferConsistent,
			      Subscribe[String, String](topics, kafkaParams)
			    )
			    // Processing
			      stream.map(record => {
			      val re560 : Double = 560.0
			      val re100 : Double  = 100.0
			      val re22 : Double  = 22.0
			      implicit val formats = org.json4s.DefaultFormats
			      println(record.value())
			      val initialData = parse(record.value()).extract[Map[String, Any]]
			      println(initialData)
			      val voltage = initialData("voltage").toString().toDouble
			      val timestamp = initialData("timestamp")
			      val current560 = (voltage/re560)*1000
			      val power560 = ((voltage*voltage)/re560)*1000
			      val current100 = (voltage/re100)*1000
			      val power100 = ((voltage*voltage)/re100)*1000
			      val current22 = (voltage/re22)*1000
			      val power22 = ((voltage*voltage)/re22)*1000
			      Map("timestamp" -> timestamp, "voltage" -> voltage,
			        "current560" -> current560,
			        "power560" -> power560,
			        "current100" -> current100,
			        "power100" -> power100,
			        "current22" -> current22,
			        "power22" -> power22)
			      
			    }).foreachRDD(EsSpark.saveToEs(_, "spark/docs"))

			    ssc.start()
			    ssc.awaitTermination()
			   
			  }
			}



e) Kibana e Elastic
	- Instalar Kibana/elastic docker: docker pull nshou/elasticsearch-kibana
	- Run: docker run -d -p 9200:9200 -p 5601:5601 nshou/elasticsearch-kibana
	- Verificar tunel ssh no pc do Dcata Collector, se ele morreu, reiniciar: 
		Code: ssh -o StrictHostKeyChecking=no -f -o ServerAliveInterval=30 -L 9092:127.0.0.1:9092 mone@0.tcp.ngrok.io -p 16364 -N 
		Senha: 'mone' ou 'moneperazzoli'

	- Limpar dados: docker stop no 'kafka' e no 'elastic'


SENHA AWS:
AAAaaa111!!!
